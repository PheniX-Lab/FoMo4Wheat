# FoMo4Wheat 
The official implementation of the paper Crop-specific Vision Foundation Model enabling Generalized Field Monitoring
# Abstract
Vision-driven in-field crop monitoring is essential for advancing digital agriculture whether supporting commercial decisions on-farm or augmenting research experiments in breeding and agronomy. Existing crop vision models struggle to generalize across fine-scale, highly variable canopy structures, and fluctuating outdoor environments. In this work, we present FoMo4Wheat, one of the first crop-orientated vision foundation models and demonstrate that delivers strong performance across a wide range of agricultural vision tasks. Centered on wheat, the most globally significant food crop, we curated ImAg4Wheat—the largest and most diverse wheat image dataset to date. It comprises 2.5 million high-resolution images collected over a decade from breeding and experimental fields, spanning more than 2,000 genotypes and 500 distinct environmental conditions across 30 global sites. A suite of FoMo4Wheat models was pre-trained using self-supervised learning on this dataset. Benchmark results across ten crop-related downstream tasks show that FoMo4Wheat consistently outperforms state-of-the-art models trained on general-domain datasets. Beyond strong cross-task generalization within wheat crops, FoMo4Wheat is highly robust in limited-data regimes but on previously unseen crop data. Notably, it contributes significantly to vision tasks in rice and multiplw crop/weed images, highlighting its cross-crop adaptability. In delivering one of the first open-source foundation models for wheat, our results demonstrate the value of such crop-specific foundation models that will support the development of versatile high-performing vision systems in crop breeding and precision agriculture. 
# Demo
(./video.mp4)
# Method
<img width="1256" height="1460" alt="image" src="https://github.com/user-attachments/assets/89b475ab-d8c3-4997-a4ec-bd4062b2f986" />
Fig 1. Overview of ImAg4Wheat dataset and FoMo4Wheat model. a, Global distribution of the ImAg4Wheat dataset. The dataset comprises 2.5 million high-resolution wheat images, captured across a wide range of genotype-by-environment combinations worldwide. b, FoMo4Wheat pre-training pipeline. The model architecture is adapted from DINOv2 and trained on the ImAg4Wheat dataset. Strong data augmentation is used to embed the image- and patch-level objectives into the teacher and student models. Model distillation driven by cross-entropy loss and feedback learning is further leveraged to guide the learning of the large and base models. c, Downstream training strategy. During adaptation to specific tasks, the pre-trained backbone is frozen, and lightweight task-specific adapters are introduced to enable efficient and flexible optimization. d, Overview of downstream task evaluation. FoMo4Wheat is fine-tuned with dedicated heads for multiple vision tasks, including classification (wheat growth stage and disease), detection (wheat spikes with ground-based and aerial imagery), counting (wheat and rice leaf), and segmentation (wheat and rice organ, multi-crop and crop-weed). e, Radar plot comparison. FoMo4Wheat consistently outperforms state-of-the-art (SOTA) models across all evaluated tasks, demonstrating superior generalization and robustness across vision tasks and crop species.
